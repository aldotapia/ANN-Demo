{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para Google Colab**\n",
    "\n",
    "Tareas:\n",
    "- Crear carpeta llamada `data` dentro de `content`\n",
    "- Subir los archivos de la carpeta `data` desde el repositorio de GitHub\n",
    "\n",
    "Instalar la sgte librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pycm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo de exploración de datos y ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from pycm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#F7F467\",\n",
    "          \"#5BF78A\",\n",
    "          \"#E1AF7D\",\n",
    "          \"#C59360\",\n",
    "          \"#FE3DFC\",\n",
    "          \"#F77C7F\",\n",
    "          \"#59C23A\",\n",
    "          \"#00F72C\",\n",
    "          \"#466D40\",\n",
    "          \"#89E14D\",\n",
    "          \"#02424B\",\n",
    "          \"#0EFCFE\",\n",
    "          \"#F7924D\",\n",
    "          \"#BFBFBF\",\n",
    "          \"#FFFD33\",\n",
    "          \"#0028FB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(path)\n",
    "files = [f for f in files if f.endswith('.csv') and f.startswith('B')]\n",
    "files = sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv(os.path.join(path, f)) for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    temp = dfs[i]\n",
    "    temp = temp.drop(temp.columns[0], axis=1)\n",
    "    if i != len(dfs) - 1:\n",
    "        temp = temp.drop(['n', 'specie'], axis=1)\n",
    "    if i == 0:\n",
    "        df = temp\n",
    "    else:\n",
    "        \n",
    "        df = pd.merge(df, temp, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df.drop(['ID', 'n'], axis=1).describe().T\n",
    "desc = desc.drop(['count'], axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(desc['min'], label='min')\n",
    "plt.plot(desc['mean'], label='mean')\n",
    "plt.plot(desc['max'], label='max')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['b02_Apr'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['b04Apr'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['b08_pen'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, stratify=df['specie'])\n",
    "train, val = train_test_split(train, test_size=0.25, stratify=train['specie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of train: {len(train)}')\n",
    "print(f'Size of val: {len(val)}')\n",
    "print(f'Size of test: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp = train.copy()\n",
    "train_temp = train_temp['specie'].value_counts()\n",
    "train_temp = train_temp.sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(train_temp.index, train_temp.values, color=colors)\n",
    "plt.title('Train data, no bootstrapping')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_or_boostrap(df, limit):\n",
    "    if len(df) > limit:\n",
    "        return df.sample(replace=False, n=limit, random_state=1)\n",
    "    else:\n",
    "        res = df\n",
    "        res2 = df.sample(n= limit - len(df), random_state=1, replace=True)\n",
    "        return pd.concat([res, res2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.groupby('specie').apply(lambda x: sample_or_boostrap(x, limit=100)).reset_index(drop=True)\n",
    "val = val.groupby('specie').apply(lambda x: sample_or_boostrap(x, limit=30)).reset_index(drop=True)\n",
    "test = test.groupby('specie').apply(lambda x: sample_or_boostrap(x, limit=30)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of train: {len(train)}')\n",
    "print(f'Size of val: {len(val)}')\n",
    "print(f'Size of test: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp = train.copy()\n",
    "train_temp = train_temp['specie'].value_counts()\n",
    "train_temp = train_temp.sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(train_temp.index, train_temp.values, color=colors)\n",
    "plt.title('Train data, no bootstrapping')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.drop(['ID','n','specie'], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train2 = pd.DataFrame(scaler.fit_transform(train2), columns=train2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = train2.describe().T\n",
    "desc = desc.drop(['count'], axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(desc['min'], label='min')\n",
    "plt.plot(desc['mean'], label='mean')\n",
    "plt.plot(desc['max'], label='max')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, n_iter=1000, metric='manhattan', random_state=42)\n",
    "train_tsne = tsne.fit_transform(train2)\n",
    "\n",
    "train_tsne = pd.DataFrame(train_tsne, columns=['c1','c2'])\n",
    "train_tsne['specie'] = train.reset_index()['specie']\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = 10,10\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "sns.scatterplot(x='c1', y='c2', hue='specie', style='specie', data=train_tsne, s = 70, alpha = 1, palette=colors, legend=\"full\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('t-SNE Bands')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = train_tsne['specie'].unique()\n",
    "\n",
    "# create new columns based on specie unique values\n",
    "for specie in species:\n",
    "    train_tsne[f'{specie}'] = 0\n",
    "\n",
    "for row in train_tsne.index:\n",
    "    temp = train_tsne.loc[row, ['c1','c2']]\n",
    "    tempspecie = train_tsne.loc[row, 'specie']\n",
    "    for specie in species:\n",
    "        temp2 = train_tsne[specie == train_tsne['specie']][['c1','c2']]\n",
    "        # exclude actual row\n",
    "        temp2 = temp2[temp2.index != row]\n",
    "        train_tsne.loc[row, specie] = manhattan_distances(temp.values.reshape(1,-1), temp2.values).min()\n",
    "\n",
    "# drop c1 and c2 columns, and pivot from almond to water to long format\n",
    "train_pivoted = train_tsne.drop(columns=['c1','c2'], axis=1).melt(id_vars='specie', var_name='specie2', value_name='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 5,5\n",
    "\n",
    "# rename specie column to Clase\n",
    "train_pivoted = train_pivoted.rename(columns={'specie':'Clase', 'distance':'Distancia'})\n",
    "\n",
    "g = sns.catplot(data=train_pivoted, x='specie2', y='Distancia',\n",
    "                col='Clase', kind='box', col_wrap=4,\n",
    "                hue='specie2', palette=colors, height=3.5, aspect=1)\n",
    "\n",
    "i = 0\n",
    "for ax in g.axes_dict.items():\n",
    "    rect = patches.Rectangle((-0.5 + i, 0), 1, 110, linewidth=1, edgecolor='none', facecolor='#F1F1F1')\n",
    "    ax[1].add_patch(rect)\n",
    "    if i >= 12:\n",
    "        ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "    i = i + 1\n",
    "g.set_xlabels('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop(['ID','n'], axis=1).to_csv('./data/train.csv', index=False)\n",
    "# val.drop(['ID','n'], axis=1).to_csv('./data/val.csv', index=False)\n",
    "# test.drop(['ID','n'], axis=1).to_csv('./data/test.csv', index=False)\n",
    "# train_backup = train.copy()\n",
    "# val_backup = val.copy()\n",
    "# test_backup = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['ID','n'], axis=1)\n",
    "val = val.drop(['ID','n'], axis=1)\n",
    "test = test.drop(['ID','n'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to int\n",
    "labels = train['specie'].unique()\n",
    "labels_dict = dict(zip(labels, range(len(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['specie'] = train['specie'].map(labels_dict)\n",
    "val['specie'] = val['specie'].map(labels_dict)\n",
    "test['specie'] = test['specie'].map(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train['specie'])\n",
    "val_labels = to_categorical(val['specie'])\n",
    "test_labels_expanded = to_categorical(test['specie'])\n",
    "test_labels = test['specie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale to min max\n",
    "scaler = MinMaxScaler()\n",
    "train_s = pd.DataFrame(scaler.fit_transform(train.drop('specie', axis=1)), columns=train.drop('specie', axis=1).columns)\n",
    "val_s = pd.DataFrame(scaler.transform(val.drop('specie', axis=1)), columns=val.drop('specie', axis=1).columns)\n",
    "test_s = pd.DataFrame(scaler.transform(test.drop('specie', axis=1)), columns=test.drop('specie', axis=1).columns)\n",
    "train_ns = train.drop('specie', axis=1)\n",
    "val_ns = val.drop('specie', axis=1)\n",
    "test_ns = test.drop('specie', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=50, mode='min')\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=10, mode='min', min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic ANN model with 1 hidden layer\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled\n",
    "history = model.fit(train_s, train_labels, validation_data=(val_s, val_labels), epochs=250, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'scaled, simple', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "# no scaled\n",
    "history2 = model.fit(train_ns, train_labels, validation_data=(val_ns, val_labels), epochs=250, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'no scaled, simple', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_acc_scaled')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_scaled')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_no_scaled')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_no_scaled')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some artificially data for no scaled data\n",
    "train_ns2 = train_ns.copy()\n",
    "train_ns2[:13] = train_ns2[:13]*0.0001 + 0.1\n",
    "train_ns2[-13:] = train_ns2[-13:]*100 - 0.5\n",
    "val_ns2 = val_ns.copy()\n",
    "val_ns2[:13] = val_ns2[:13]*0.0001 + 0.1\n",
    "val_ns2[-13:] = val_ns2[-13:]*100 - 0.5\n",
    "test_ns2 = test_ns.copy()\n",
    "test_ns2[:13] = test_ns2[:13]*0.0001 + 0.1\n",
    "test_ns2[-13:] = test_ns2[-13:]*100 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "# no scaled with artificially data\n",
    "history2 = model.fit(train_ns2, train_labels, validation_data=(val_ns2, val_labels), epochs=250, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'no scaled, simple, exagerated', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_acc_scaled')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_scaled')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_no_scaled')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_no_scaled')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic ANN model with 1 hidden layer + dropout\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(train_s, train_labels, validation_data=(val_s, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'scaled, simple with dropout', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_1lyr')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_1lyr')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_1lyr+dpt')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_1lyr+dpt')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic ANN model with 1 hidden layer + sigmoid\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],)))\n",
    "    model.add(Dense(128, activation='sigmoid'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model.fit(train_s, train_labels, validation_data=(val_s, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'scaled, simple with dropout + sigmoid', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_1lyr')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_1lyr')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_1lyr+dpt')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_1lyr+dpt')\n",
    "plt.plot(history3.history['accuracy'], label='train_acc_1lyr+dpt+sigmoid')\n",
    "plt.plot(history3.history['val_accuracy'], label='val_acc_1lyr+dpt+sigmoid')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic ANN model with 1 hidden layer + tanh\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],)))\n",
    "    model.add(Dense(128, activation='tanh'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = model.fit(train_s, train_labels, validation_data=(val_s, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'scaled, simple with dropout + tanh', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_1lyr')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_1lyr')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_1lyr+dpt')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_1lyr+dpt')\n",
    "plt.plot(history3.history['accuracy'], label='train_acc_1lyr+dpt+sigmoid')\n",
    "plt.plot(history3.history['val_accuracy'], label='val_acc_1lyr+dpt+sigmoid')\n",
    "plt.plot(history4.history['accuracy'], label='train_acc_1lyr+dpt+tanh')\n",
    "plt.plot(history4.history['val_accuracy'], label='val_acc_1lyr+dpt+tanh')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep ANN model with 2 hidden layers + relu\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(train_s, train_labels, validation_data=(val_s, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'scaled, 2hl', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_ann')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_ann')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_dnn')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_dnn')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get bigger\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model.fit(train_s, train_labels, validation_data=(val_s, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'scaled, 3hl', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_ann')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_ann')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_dnn')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_dnn')\n",
    "plt.plot(history3.history['accuracy'], label='train_acc_dnn_bigger')\n",
    "plt.plot(history3.history['val_accuracy'], label='val_acc_dnn_bigger')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 layers, but more neurons\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],)))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history4 = model.fit(train_s, train_labels, validation_data=(val_s, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s, test_labels_expanded)\n",
    "l.append({'model': 'scaled, 3hl +neurons', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_ann')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_ann')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_dnn')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_dnn')\n",
    "plt.plot(history3.history['accuracy'], label='train_acc_dnn_bigger')\n",
    "plt.plot(history3.history['val_accuracy'], label='val_acc_dnn_bigger')\n",
    "plt.plot(history4.history['accuracy'], label='train_acc_dnn_bigger_more')\n",
    "plt.plot(history4.history['val_accuracy'], label='val_acc_dnn_bigger_more')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to deep learning!\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],1)))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s2 = train_s.to_numpy().reshape(train_s.shape[0], train_s.shape[1],1)\n",
    "val_s2 = val_s.to_numpy().reshape(val_s.shape[0], val_s.shape[1],1)\n",
    "test_s2 = test_s.to_numpy().reshape(test_s.shape[0], test_s.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history2 = model.fit(train_s2, train_labels, validation_data=(val_s2, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s2, test_labels_expanded)\n",
    "l.append({'model': 'scaled, dl', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_ann')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_ann')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_dl')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_dl')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to deep learning! ver 2, bigger\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],1)))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history3 = model.fit(train_s2, train_labels, validation_data=(val_s2, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s2, test_labels_expanded)\n",
    "l.append({'model': 'scaled, dl v2', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_ann')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_ann')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_dl')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_dl')\n",
    "plt.plot(history3.history['accuracy'], label='train_acc_dl_v2')\n",
    "plt.plot(history3.history['val_accuracy'], label='val_acc_dl_v2')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to deep learning! ver 3, longer\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],1)))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(16, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history4 = model.fit(train_s2, train_labels, validation_data=(val_s2, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s2, test_labels_expanded)\n",
    "l.append({'model': 'scaled, dl v3', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_ann')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc_ann')\n",
    "plt.plot(history2.history['accuracy'], label='train_acc_dl')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc_dl')\n",
    "plt.plot(history3.history['accuracy'], label='train_acc_dl_v2')\n",
    "plt.plot(history3.history['val_accuracy'], label='val_acc_dl_v2')\n",
    "plt.plot(history4.history['accuracy'], label='train_acc_dl_v3')\n",
    "plt.plot(history4.history['val_accuracy'], label='val_acc_dl_v3')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to deep learning! with fancy stuff to control overfitting\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],1)))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(16, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history5 = model.fit(train_s2, train_labels, validation_data=(val_s2, val_labels), epochs=300, batch_size=32, callbacks=[early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s2, test_labels_expanded)\n",
    "l.append({'model': 'scaled, dl v3 + BN', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history4.history['accuracy'], label='train_acc_dl_v3')\n",
    "plt.plot(history4.history['val_accuracy'], label='val_acc_dl_v3')\n",
    "plt.plot(history5.history['accuracy'], label='train_acc_dl_v3_bn')\n",
    "plt.plot(history5.history['val_accuracy'], label='val_acc_dl_v3_bn')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to deep learning! with fancy stuff to control overfitting\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_s.shape[1],1)))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(16, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history = model.fit(train_ns, train_labels, validation_data=(val_ns, val_labels), epochs=300, batch_size=32, callbacks=[reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ns, test_labels_expanded)\n",
    "l.append({'model': 'no scaled, dl v4', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history = model.fit(train_s2, train_labels, validation_data=(val_s2, val_labels), epochs=300, batch_size=32, callbacks=[reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_s2, test_labels_expanded)\n",
    "l.append({'model': 'scaled, dl v4', 'test_loss': test_loss, 'test_acc': test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.argmax(test_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(np.array(test_labels), test_pred, digit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.relabel(mapping={0: 'almond', 1: 'avocado', 2: 'barren', 3: 'barren shadowed', 4: 'forage',\n",
    "                    5: 'industrial grape', 6: 'lemon', 7: 'mandarin', 8: 'olive', 9: 'orange',\n",
    "       10: 'riverside vegetation', 11: 'short cycle crop', 12: 'table grape', 13: 'urban',\n",
    "       14: 'walnut', 15: 'water'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 10,10\n",
    "\n",
    "\n",
    "cm.plot(cmap=plt.cm.CMRmap_r, number_label=True, plot_lib=\"matplotlib\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Matriz de confusión para el mejor modelo')\n",
    "plt.ylabel('Clase observada')\n",
    "plt.xlabel('Clase predicha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.Overall_ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.KappaUnbiased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
